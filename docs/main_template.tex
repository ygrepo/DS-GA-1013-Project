\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}
\PassOptionsToPackage{numbers}{natbib}
% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage{amsmath,amssymb,bm}
\usepackage{caption}
\captionsetup[figure]{labelfont=small, justification=centering}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{lmodern}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{etoolbox}

\usepackage[dvipsnames]{xcolor}
\usepackage[hang,flushmargin]{footmisc}
\usepackage{bbm}
\usepackage{pgfplots}
\usetikzlibrary{spy}
\usepgfplotslibrary{external}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{arrows,automata}
\usepackage{thmtools}
\usepackage{blkarray} 
\usepackage{textcomp}

\usepackage{times}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{color}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{amstext}
\usepackage{blkarray}
\usepackage{url}
\usepackage{epsfig}
\usepackage{bm}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{minted}


\title{Learning to regularize, Application of Neumann Network to the fastMRI dataset}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.
\author{%
  Yves Greatti \\
  New York University\\
  % examples of more authors
   \And
    Soham Girish Tamba \\
    New York University\\
}

\input{macros}
\begin{document}

\maketitle

\begin{abstract}
 Recent progress in Machine Learning and Data science has seen the development of efficient Neural Networks architecture to solve inverse-problems. Neural networks have surpassed more traditional approaches in 
  many natural sciences, medicine and life sciences, challenging applications.
 Our goal in this work has been first to perform a quick survey of data-driven inverse problem methodologies, followed by an understanding of the key ideas at the core of a Neumann Network and its implementation
 and application to the task of image reconstruction using the fastMRI dataset, a very-large collection of  undersampled Magnetic Resonance Imaging (MRI) measurements. 
  \end{abstract}
  
 \section{Introduction}
\label{Introduction}

In term of operator, an inverse problem is formalized as solving an operator equation
\begin{equation}
	\matr{Y} = \mathcal{A} (\beta_{\text{true}}) + \epsilon \label{eq:1}
\end{equation}
where $\mathcal{A}$ is a forward operator from the model parameter space $\matr{X}$ to the data space $\matr{Y}$,  $\beta_{\text{true}}$ the ground truth image and $\epsilon$ a random variable modeling the observation noise.
 The MRI (Magnetic Resonance Imaging) image reconstruction problem is an inverse problem, the forward operator is a discrete sampling operator concatenated with the Fourier transform.
 To eliminate image artifacts due to patient movements, an MRI machine acquires less data to reduce the scan time.
For the fastMRI image reconstruction challenge, the undersampling consists  in masking k-space lines from a fully-sampled acquisition.
A difficulty in solving  \eqref{eq:1}, is that the solution is sensitive to variations in the data, which is referred to ill-posedness. The notion of  ill-posedness is attributed to Hadamard, who defined that, by contrast, a well-posed problem
has three distinct properties: (1) it has a solution, that is unique (2), and that depends continuously on the data (3). Another way of describing  ill-posedness is to understand the operator  $\mathcal{A}$ and when it is linear , look at its singular
value decomposition. The decay of the spectral spectrum $\sigma_k, k \in \mathcal{N}$ of  $\mathcal{A}$ is strongly related to the ill-posedness: faster decay implies a more ill-posed problem. 
 	
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=200pt]{variance_plot_log.pdf}
		\caption{$50$ first singular value of the k-space of an image from the fastMRI dataset after undersampling.}
	\end{figure}
The problem defined in \eqref{eq:1}, can be formulated as an ordinary least squares problem (OLS) , given a set of training images $\{\vect{y_1}, \vect{y_2}, \ldots, \vect{y_n}\}$, and an operator $\mathcal{A}$, we want to compute the estimates 
$\vect{\beta}$ which satisfy:
$$
\hat{\vect{\beta}}  =  \argmin_{\beta} \| \vect{y} -\mathcal{A} \vect{\beta} \|_2^2
$$
The aim of regularization theory is to provide a regularizer $r(\cdot)$ which pushes the estimate $\vect{\beta}$ to $\vect{\beta}_{\text{true}}$. We now set the inverse problem as:
$$
\hat{\vect{\beta}}  =  \argmin_{\beta} \frac{1}{2} \| \vect{y} -\mathcal{A} \vect{\beta} \|_2^2 + r(\vect{\beta})
$$
The regularization operator $r(\cdot)$, is a denoising encoder and one approach to solve these inverse problems is to learn $r(\cdot)$. 
 
\section{Theory}
\label{Theory}

The core idea of a Neumann network, it is to use a denoising autoencoder as a proximal operator in an iterative scheme.
The estimated reconstructed image is given by:
\begin{equation}
\op{prox}_{r}\brac{\vect{\beta}} := \arg \min_{\vect{\beta}} r\brac{\vect{\beta}}+ \frac{1}{2} \normTwo{\vect{y}-\matr{X}\vect{\beta}}^2  \label{eq:2}
\end{equation}

Assuming that $r\brac{\vect{\beta}} = \frac{1}{2} \vect{\beta}^T \vect{R} \vect{\beta}$ then taking the gradient of the \eqref{eq:2}, leads to the optimal solution $\vect{\beta}^{\ast}$
\begin{align*}
	(\matr{X}^T \matr{X} + \matr{R}) \matr{\beta}^* &=	\matr{X}^T \vect{y} \\
	\matr{\beta}^* 			&= (\matr{X}^T \matr{X} + \matr{R})^{-1} \matr{X}^T \vect{y} \\
\end{align*}
Using a well-established  result,  for any matrix, $ p \times p \, \matr{A}$, the Neumann series, $\sum_{k=0}^\infty \matr{A}^k$, converges to the inverse of $\matr{I} - \matr{A}$,
assuming $\|\matr{A}\|_{l_i} < 1$ where $l_i$ is an operator norm. We are now in position to replace the inversion defined in  \eqref{eq:2}, by a finite sum of power of $\matr{A}$:
\begin{align*}
	(\matr{I} - \matr{A})^{-1}	&= \sum_{k=0}^{\infty} \matr{A}^k = \matr{I} + \matr{A} + \matr{A}^2 + \matr{A}^3 + \cdots \\ 
	\matr{B}^{-1}			&= \eta  \sum_{k=0}^{\infty}  (\matr{I} - \eta \matr{B})^k \text{~ ancillary result} \\	
	\matr{\beta}^*			&=  \sum_{j=0}^{\infty} (\matr{I} - \eta \matr{X}^T \matr{X} -\eta \matr{R})^j (\eta \matr{X}^T \vect{y}) \\
	\hat{\matr{\beta}}(\vect{y})  &:=  \sum_{j=0}^{B} \bigg([ \matr{I} - \eta \matr{X}^T \matr{X} ](\cdot) -\eta \matr{R}(\cdot) \bigg)^j (\eta \matr{X}^T \vect{y}) \\	
\end{align*}
The last equation has a natural recursive form which is easily interpretable as a deep learning architecture:
\begin{align*}
	\tilde{\vect{\beta}}^{(j)} 		&:=	\bigg( \matr{I} - \eta \matr{X}^T \matr{X} \bigg) \tilde{\vect{\beta}}^{(j-1)} - \eta R(\tilde{\vect{\beta}}^{(j-1)}), j=1, \cdots , B \\
	\tilde{\vect{\beta}}(\vect{y})	&:=   \sum_{j=0}^{B} \tilde{\vect{\beta}}^{(j)} \\
\end{align*}
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=400pt]{nn_architecture}
		\caption{Neumann Network Architecture}
	\end{figure}
One interesting lemma established by Gilton et Al, is that the finite sum $\sum_{j=0}^{B} \tilde{\vect{\beta}}^{(j)} $ provides indeed an estimate for the reconstructed image $\tilde{\vect{\beta}}(\vect{y})$  approximatvely equal to the optimal reconstruction  $\vect{\beta}^{\ast}$, almost \emph{independently} of the number of iterations, which is a result we recovered through our experiments.

Assuming that the autoencoder is linear with the input 
$$
	\vect{R}(\matr{\beta}) = \vect{R} \matr{\beta} = -c_{\eta,B} ( \matr{I} - \matr{X}^T \matr{X}) 
	\matr{U} (\matr{U}^T   \matr{X}^T \matr{X}  \matr{U})^{-1} \matr{U}^T   \matr{X}^T \matr{X} 
$$ where $\tilde{\vect{\beta}}^{(0)} = \eta \matr{X}^T \vect{y}, \vect{y} = \matr{X} \vect{\beta}^{\ast}$

then
\begin{align*}
	\tilde{\vect{\beta}}^{(j)}			&=  a_j \matr{X}^T \matr{X} \vect{\beta}^* + b_j (\matr{I} - \matr{X}^T \matr{X} ) \vect{\beta}^* \\
		&\text{ where } \sum_j a_j \approx 1 \text{ and }  \sum_j b_j \approx 1  \\
	\hat{\vect{\beta}}(\vect{y})	&= \sum_{j=0}^B \tilde{\vect{\beta}}^{(j)} \\
						&\approx \matr{X}^T \matr{X}  \vect{\beta}^* + (\matr{I} - \matr{X}^T \matr{X} ) \vect{\beta}^* =   \vect{\beta}^* \\
\end{align*}


\section{MRI Experiments}
\label{MRIExperiments}

PLEASE START HERE DESCRIPTION OF FAST MRI 


  
%\bibliographystyle{plainnat}
%\bibliography{references}
\end{document}

